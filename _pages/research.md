---
layout: single
title: Research
permalink: /research/
classes: wide
description: "Find out more about Vandad Davoodnia (Ph.D.) and his research on his personal website. He is interested in computer vision, deep learning, generative AI, and virtual worlds. His most recent research is on modeling virtual humans using cameras."
---

I'm interested in computer vision, deep learning, generative AI, and virtual worlds.
My most recent research is on modeling virtual humans using cameras. Here is
a summary of my research:

<html lang="en">
    <table style="width:100%;height:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;overflow:hidden;">
    <tbody>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
            </td>
            <td width="75%" valign="middle">
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/upose3d.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
                    <papertitle>UPose3D: uncertainty-aware 3D human pose estimation with cross-view and temporal cues</papertitle>
                <br>
                <paperauthor>
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://saeed1262.github.io/">Saeed Ghorbani</a>,
                    <a href="https://macarbonneau.github.io/">Marc-Andr√© Carbonneau</a>,
                    Alexandre Messier,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>arXiv</em>, 2024
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We present an uncertainty-aware 3D pose estimation method through maximizing the likelihood of 3D
                    keypoints based on the differentiable distributions of multi-view 2D keypoints.
                </p>
                <a href="https://doi.org/10.48550/arXiv.2404.14634">[Paper]</a> <a href="https://vdavoodnia.github.io/projects/upose3d">[Project]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/skelformer.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.2404.14634">-->
                    <papertitle>SkelFormer: markerless 3D pose and shape estimation using skeletal transformers</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://saeed1262.github.io/">Saeed Ghorbani</a>,
                    Alexandre Messier,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>arXiv</em>, 2024
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We present SkelFormer, an inverse-kinematic regression framework for estimating human pose and shape from videos.
                </p>
                <a href="https://doi.org/10.48550/arXiv.2404.14634">[Paper]</a> <a href="https://vdavoodnia.github.io/projects/skelformer">[Project]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/FabriCar.png"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.1145/3563657.3595988">-->
                    <papertitle>FabriCar: enriching the user experience of in-car media interactions with ubiquitous vehicle interiors using e-textile sensors</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    Pouya M Khorsandi,
                    <a href="https://leejones.ca/">Lee Jones</a>,
                    <strong>Vandad Davoodnia</strong>,
                    Timothy J Lampen,
                    Aliya Conrad,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>,
                    <a href="https://istudio.cs.queensu.ca/">Sara Nabil </a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>ACM Designing Interactive Systems Conference (DIS)</em>, 2023
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We investigate the use of e-textiles in cars to reduce driver distraction when performing non-driving tasks like media playback.
                </p>
                <a href="https://doi.org/10.1145/3563657.3595988">[Paper]</a> <a href="https://leejones.ca/s/dis23-30-compressed.pdf">[PDF]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/icassp_transformer.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.2303.05691">-->
                    <papertitle>Human pose estimation from ambiguous pressure recordings with spatio-temporal masked transformers</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We propose a temporal variation of the ViTs for pose estimation and pre-train it using a masked
                    auto-encoder framework.
                </p>
                <a href="https://doi.org/10.48550/arXiv.2303.05691">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/pressure_bmi.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.2006.10453">-->
                    <papertitle>Deep multitask learning for pervasive BMI estimation and identity recognition in smart beds</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <strong>Vandad Davoodnia</strong>,
                    Monet Slinowsky,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em> Journal of Ambient Intelligence and Humanized Computing</em>, 2020
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We use neural networks for BMI estimation and user identification from in-bed pressure sensors.
                </p>
                <a href="https://doi.org/10.48550/arXiv.2006.10453">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/parse.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.2202.05400">-->
                    <papertitle>Parse: Pairwise alignment of representations in semi-supervised eeg learning for emotion recognition</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <a href="https://www.guangyizhang.com/">Guangyi Zhang</a>,
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>IEEE Transactions on Affective Computing</em>, 2022
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We present PARSE for understanding emotions from EEG data when limited labeled data is available.
                </p>
                <a href="https://doi.org/10.48550/arXiv.2202.05400">[Paper]</a> <a href="https://github.com/guangyizhangbci/PARSE">[Code]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/polishnetU.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.2206.06518">-->
                    <papertitle>Estimating pose from pressure data for smart beds with deep image-based pose estimators</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://saeed1262.github.io/">Saeed Ghorbani</a>,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>Journal of Applied Intelligence</em>, 2021
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We present a domain adaptation method to improve body pose detection from pressure data.
                </p>
                <a href="https://doi.org/10.48550/arXiv.2206.06518">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/pressure_icassp.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.1908.08919">-->
                    <papertitle>In-bed pressure-based pose estimation using image space representation learning</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://saeed1262.github.io/">Saeed Ghorbani</a>,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2021
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We address the problems of using standard pose estimation models on pressure maps by
                    transforming unclear pressure maps into a format resembling human figures on normal images.
                </p>
                <a href="https://doi.org/10.48550/arXiv.1908.08919">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/cancer.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.1109/ICMLA.2019.00077">-->
                    <papertitle>Computer-aided diagnosis using class-weighted deep neural network</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <a href="https://pritamsarkar.com/">Pritam Sarkar</a>,
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>IEEE International Conference On Machine Learning And Applications (ICMLA)</em>, 2019
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We present a deep learning method for diagnosing breast cancer tumors from medical images.
                </p>
                <a href="https://doi.org/10.1109/ICMLA.2019.00077">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/recursive_bilstm.jpg" alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://github.com/guangyizhangbci/A-Novel-Recursive-Network-for-Irony-Detection-in-Tweets/blob/main/doc/project_report.pdf">-->
                    <papertitle>A novel recursive network for irony detection in tweets</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <a href="https://www.linkedin.com/in/arthur-araujoo">Arthur Cruz de Araujo</a>
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://www.guangyizhang.com/">Guangyi Zhang</a>,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>,
                    <a href="https://www.xiaodanzhu.com/">Xiaodan Zhu</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>Course Report</em>, 2019
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We present a recursive attention-based Bi-LSTM neural network for
                    irony detection in tweets.
                </p>
                <a href="https://github.com/guangyizhangbci/A-Novel-Recursive-Network-for-Irony-Detection-in-Tweets/blob/main/doc/project_report.pdf">[Paper]</a>
                <a href="https://github.com/guangyizhangbci/A-Novel-Recursive-Network-for-Irony-Detection-in-Tweets">[Code]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/eeg_movement.jpg"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.1908.02252">-->
                    <papertitle>Classification of hand movements from EEG using a deep attention-based LSTM network</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <a href="https://www.guangyizhang.com/">Guangyi Zhang</a>,
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://arsm.github.io/">Alireza Sepas-Moghaddam</a>,
                    Yaoxue Zhang,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>IEEE Sensors Journal</em>, 2020
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We present a left and right hand movements classification method from EEG signals using
                    LSTM network with added attention mechanism.
                </p>
                <a href="https://doi.org/10.48550/arXiv.1908.02252">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/smc2019.png"  alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.48550/arXiv.2104.02159">-->
                    <papertitle>Identity and posture recognition in smart beds with deep multitask learning</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    <strong>Vandad Davoodnia</strong>,
                    <a href="https://www.aiimlab.com/ali-etemad/">Ali Etemad</a>
                </paperauthor>
                <br>
                <paperpublication>
                    <em>IEEE International Conference on Systems, Man and Cybernetics (SMC)</em>, 2019
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We introduce a deep learning model that uses pressure sensor arrays to accurately identify
                    a person and their sleep posture.
                </p>
                <a href="https://doi.org/10.48550/arXiv.2104.02159">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/anfis.jpg" alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.1016/j.fuel.2019.04.169">-->
                    <papertitle>Prognostication of lignocellulosic biomass pyrolysis behavior using ANFIS model tuned by PSO algorithm</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    Mortaza Aghbashlo, Meisam Tabatabaei, Mohammad Hossein Nadian, <strong>Vandad Davoodnia</strong>, Salman Soltanian
                </paperauthor>
                <br>
                <paperpublication>
                    <em>Fuel</em>, 2019
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We developed an Adaptive Network-based Fuzzy Inference System (ANFIS) model to estimate biomass
                    pyrolysis behavior and trained it using	Particle Swarm Optimization (PSO).
                </p>
                <a href="https://doi.org/10.1016/j.fuel.2019.04.169">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="/assets/images/fractals.png" alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
            </td>
            <td width="75%" valign="middle">
<!--				<a href="https://doi.org/10.1016/j.jneumeth.2018.10.039">-->
                    <papertitle>Multifractal detrended fluctuation analysis of continuous neural time series in primate visual cortex</papertitle>
<!--				</a>-->
                <br>
                <paperauthor>
                    Zahra Fayyaz, Mohammadreza Bahadorian, Jafar Doostmohammadi, <strong>Vandad Davoodnia</strong>, Sajad Khodadadian, Reza Lashgari
                </paperauthor>
                <br>
                <paperpublication>
                    <em>Journal of Neuroscience Methods</em>, 2019
                </paperpublication>
                <br>
                <p style="font-size:small" align="justify">
                    We demonstrated a new method for response tuning of neural activity time-series.
                </p>
                <a href="https://doi.org/10.1016/j.jneumeth.2018.10.039">[Paper]</a> <!-- Replace with actual GitHub repo link -->
            </td>
        </tr>
        <tr class="paper-info">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="/assets/images/eye_speed.webp" alt="clean-usnob" class="center"  style="width:240px;height:160px;object-fit:contain;">
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Effect of age and glaucoma on the detection of darks and lights</papertitle>
                    <br>
                    <paperauthor>
                        Linxi Zhao, Caroline Sendek, <strong>Vandad Davoodnia</strong>, Reza Lashgari, Mitchell W Dul, Qasim Zaidi, Jose-Manuel Alonso
                    </paperauthor>
                    <br>
                    <paperpublication>
                        <em>Investigative Ophthalmology & Visual Science (IOVS)</em>, 2015
                    </paperpublication>
                    <br>
                    <p style="font-size:small" align="justify">
                        We developed an iOS and Android App to study the effect of age and glaucoma on
                        response time to light and dark targets.
                    </p>
                    <a href="https://doi.org/10.1167/iovs.15-16753">[Paper]</a> <!-- Replace with actual GitHub repo link -->
                    <a href="https://apps.apple.com/ca/app/eye-speed/id1033931375">[iOS]</a>
                    <a href="https://apkpure.com/eye-speed/vandad.testtrial">[Android]</a>
                </td>
            </tr>
    </tbody>
    </table>
</html>

